{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/LL3-backdrop.png\">\n",
    "\n",
    "### Dates/Deadlines\n",
    "\n",
    "Code: <i>29/04/2019 (one week before submission)</i><br />\n",
    "Report: <i>05/05/2019 (20:00)</i><br />\n",
    "\n",
    "### Imports\n",
    "\n",
    " You'll need to install the following libraries:\n",
    " * Gym (should have it by now)\n",
    " * Box2D\n",
    "  * brew install swig\n",
    "  * pip install box2d-py\n",
    "  * pip install box2d\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    " * Should we do [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2/) or [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2/)?\n",
    "\n",
    "\n",
    "### Comments\n",
    " \n",
    " * I think we should start with the discrete [LunarLander](https://gym.openai.com/envs/LunarLander-v2/) first then if we have the time attempt the continous one.\n",
    " \n",
    "### Report\n",
    "\n",
    "The project report should describe the problem you address, present your approach (or\n",
    "approaches, if you experimented with more than one approach), and evaluate how well you have\n",
    "solved the problem. It should also discuss alternative solution methods that are applicable, along\n",
    "with the relative merits of your approach. Finally, the report should describe your personal\n",
    "experience with the project, for example, the difficulties or any pleasant surprises you have\n",
    "encountered along the way. Do not explain material that has been discussed in the lectures. For\n",
    "example, there is no need to explain Q-learning or function approximation. \n",
    "\n",
    "Page limit: 8\n",
    "\n",
    "### TO-DO\n",
    "\n",
    " * Create the agent\n",
    " * Save videos after $n$ iterations\n",
    " * Look into including the Neural Network like Mar.io (not important) \n",
    " * We need to think of some \"alternative solution\" to this problem\n",
    "\n",
    " \n",
    "### Links\n",
    "\n",
    " * [GitHub](https://github.bath.ac.uk/jwm59/RL-Robotics-Project)\n",
    " * [Trouble install Box2D - helped me](https://github.com/openai/gym/issues/100)\n",
    " * [More trouble shooting with Box2D](http://www.jtdz-solenoids.com/stackoverflow_/questions/44198228/install-pybox2d-for-python-3-6-with-conda-4-3-21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output, display #Not sure what this is doing exactly\n",
    "\n",
    "import Box2D #Not sure we need this, check later\n",
    "\n",
    "\n",
    "#####\n",
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the gym environment\n",
    "\n",
    "## LunarLander-v2\n",
    "\n",
    "* Observation Space: 8\n",
    " * X\n",
    " * Y\n",
    " * X velocity\n",
    " * Y velocity\n",
    " * Self angle\n",
    " * Angular velocity\n",
    " * Left leg contact \n",
    " * Right lef contact\n",
    "* Action Space: 4\n",
    " * Do nothing\n",
    " * Fire left orientation engine\n",
    " * Fire right orientation engine\n",
    " * Fire main engine\n",
    "\n",
    "\n",
    "## LunarLanderContinuous-v2\n",
    "\n",
    "* Observation Space: 8\n",
    " * X\n",
    " * Y\n",
    " * X velocity\n",
    " * Y velocity\n",
    " * Self angle\n",
    " * Angular velocity\n",
    " * Left leg contact \n",
    " * Right lef contact\n",
    "* Action Space: two real values vector from -1 to +1\n",
    " * First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power\n",
    " * Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_discrete = gym.make('LunarLander-v2')\n",
    "env_continuous = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "### Approach\n",
    "\n",
    "\"Probably should explain the approach we plan to take...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Deep-Q network\n",
    "def q_network(numInput, numHidden, numHiddenLayers, numOutput, optimiserFunction=tf.train.AdamOptimizer, \\\n",
    "              alpha=0.001, lossFunction=\"mse\", hiddenActivation=\"relu\", outputActivation=\"linear\"): \n",
    "    \n",
    "    #Creating a TensorFlow class\n",
    "    network = Sequential()\n",
    "    \n",
    "    #Creating first hidden layer\n",
    "    network.add(Dense(numHidden, input_dim=numInput, activation=hiddenActivation))\n",
    "    \n",
    "    #Adding 'n' hidden layers\n",
    "    for _ in range(numHiddenLayers):\n",
    "        network.add(Dense(numHidden, activation=hiddenActivation))\n",
    "        \n",
    "    #Creating output layer\n",
    "    network.add(Dense(numOutput, activation=outputActivation))\n",
    "    \n",
    "    #Defining the loss function, the optimiser and the metrics.\n",
    "    network.compile(loss=lossFunction, optimizer=Adam(lr=alpha))\n",
    "    \n",
    "    return network   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Full agent class. Look back on the w\n",
    "class rocketMan_V2():\n",
    "    def __init__(self, environment, numObservations, numActions, numHidden, numHiddenLayer, modelFileName, \\\n",
    "                     gamma=0.99, alpha=0.0001, epsilon=1, epsilonDecay=0.995, epsilon_min=0.01, tau=0.95, \\\n",
    "                     load=False, save=True):\n",
    "\n",
    "        self.numObservations = numObservations\n",
    "        self.numActions = numActions\n",
    "        self.numHidden = numHidden\n",
    "        self.numHiddenLayer = numHiddenLayer\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilonDecay = epsilonDecay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        #replay buffer\n",
    "        self.replay_buffer = []\n",
    "        \n",
    "        # The networks\n",
    "        self.q_network = q_network(self.numObservations,self.numHidden,self.numHiddenLayer,self.numActions)\n",
    "        self.q_target_network = q_network(self.numObservations,self.numHidden,self.numHiddenLayer,self.numActions)\n",
    "                \n",
    "        if(load):\n",
    "            self.q_network.load_weights(modelFileName)\n",
    "        \n",
    "        self.soft_target_weight_update(1)\n",
    "        \n",
    "        ##### HISTORY\n",
    "        self.deck = deque(maxlen=2000)\n",
    "        \n",
    "    def updateEpsilon(self):\n",
    "        '''\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilonDecay\n",
    "        \n",
    "        if(self.epsilon < self.epsilon_min):\n",
    "            self.epsilon = self.epsilon_min\n",
    "        '''\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilonDecay*self.epsilon)\n",
    "    \n",
    "    def select_epsilon_greedy_action(self, state):            \n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return(env.action_space.sample())  # random action\n",
    "        else:\n",
    "            q_values = self.q_network.predict(state)\n",
    "            return(np.argmax(q_values[0]))        # greedy action\n",
    "\n",
    "    def soft_target_weight_update(self, tau):\n",
    "        # Q network weights\n",
    "        weights = np.asarray(self.q_network.get_weights())\n",
    "        \n",
    "        # Target network weights\n",
    "        target_weights = np.asarray(self.q_target_network.get_weights())\n",
    "        \n",
    "        # \n",
    "        self.q_target_network.set_weights( (weights * tau) + (target_weights * (1 - tau)))\n",
    "\n",
    "    def learn_from_transition(self, action, state,next_state, reward, done):\n",
    "        '''\n",
    "        q_values = self.q_network.predict(state)\n",
    "\n",
    "        target_q_values = q_values\n",
    "        \n",
    "        if done == True: \n",
    "            target_q_values[0][action] = reward\n",
    "        else: \n",
    "            target_q_values[0][action] = reward + (self.gamma * np.max(self.q_target_network.predict(next_state)) )\n",
    "        \n",
    "        self.q_network.fit(state, target_q_values, epochs=8, batch_size=32, verbose=0)\n",
    "        '''\n",
    "        target = reward\n",
    "        if not done: #((1-ALPHA)*xreward)+ (ALPHA* (GAMMA * futurereward))\n",
    "            target = ( (1.0-0.1)*reward + 0.1 * (self.gamma*np.amax(self.q_network.predict(next_state)[0])))                \n",
    "\n",
    "        target_old = self.q_network.predict(state)\n",
    "        target_old[0][action] = target\n",
    "        # Train\n",
    "        #K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_‌​parallelism_threads=‌​32, inter_op_parallelism_threads=32)))\n",
    "        self.q_network.fit(x=state, y=target_old,\\\n",
    "                            #batch_size=1,\\\n",
    "                            verbose=0,\\\n",
    "                            epochs=1)\n",
    "\n",
    "    def learn_from_m_random_transitions_in_replay_buffer(self, batch=16):\n",
    "        '''\n",
    "        assert m >= 1\n",
    "        \n",
    "        size_of_replay_buffer = len(self.replay_buffer)\n",
    "        \n",
    "        sortTest = sorted(self.replay_buffer, reverse = True, key=lambda tup: tup[3])\n",
    "        \n",
    "        for _ in range(m):                          # m is number of transitions\n",
    "            transition_index = random.randint(0, size_of_replay_buffer-1)         \n",
    "            action, state, next_state, reward, done_bool = self.replay_buffer[-(transition_index+1)]\n",
    "            self.learn_from_transition(action, state, next_state, reward, done_bool)    \n",
    "        '''\n",
    "        sample_indx = random.sample(self.deck, batch)\n",
    "        \n",
    "        for action, current_state, next_state, reward, done in sample_indx: \n",
    "            self.learn_from_transition(action, current_state, next_state, reward, done)\n",
    "            \n",
    "        self.soft_target_weight_update(0.5)    \n",
    "        \n",
    "    def add_to_replay_buffer(self, action, current_state, next_state, reward, done): \n",
    "        transition_tuple = (action, current_state, next_state, reward, done)\n",
    "        self.replay_buffer.append(transition_tuple)\n",
    "        \n",
    "        self.deck.append(transition_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Basic\n",
    "\n",
    "Before looking into Experience Replay and Target Networks, I thought it would be best to implement a basic model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9d61fc418f41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeanRewards1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanRewards1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"20 mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             _png.write_png(renderer._renderer, fh,\n\u001b[0;32m--> 523\u001b[0;31m                             self.figure.dpi, metadata=metadata)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://github.com/RMiftakhov/LunarLander-v2-drlnd\n",
    "\n",
    "env = env_discrete\n",
    "\n",
    "numObservation = env.observation_space.shape[0]\n",
    "numActions = env.action_space.n\n",
    "numHidden = 128\n",
    "numHiddenLayer = 1 #Already has a hidden layer\n",
    "\n",
    "modelFileName = \"LunarLanderWeights.h5\"\n",
    "\n",
    "agent = rocketMan_V2(env, numObservation, numActions, numHidden, numHiddenLayer, modelFileName,\n",
    "                    save=True, load=False)\n",
    "starting_state = env.reset() # Getting things ready\n",
    "\n",
    "num_episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "reward_array = np.ndarray(shape = (num_episodes, max_steps))\n",
    "steps_array = np.ndarray(shape = (num_episodes))\n",
    "\n",
    "rewardList = []\n",
    "meanRewards = []\n",
    "meanRewards1 = []\n",
    "episodeList = []\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_window1 = deque(maxlen=20)\n",
    "for episode in range(num_episodes):\n",
    "    step = 1\n",
    "    reward_sum = 0\n",
    "    \n",
    "    # reseting the environment and getting the starting episode action. \n",
    "    current_state = env.reset().reshape(1, numObservation)\n",
    "    chosen_action = agent.select_epsilon_greedy_action(current_state)    \n",
    "    \n",
    "# The main loop for a given episode ------------------------------\n",
    "    done = False\n",
    "    #while done == False and step < max_steps:   # The problem is here:\n",
    "    while done == False:\n",
    "            \n",
    "        next_state, reward, done, info = env.step(chosen_action)\n",
    "        next_state = next_state.reshape(1, numObservation)  # Tedious reshaping needed.\n",
    "        \n",
    "        agent.add_to_replay_buffer(chosen_action, current_state, next_state, reward, done)\n",
    "        \n",
    "        # Setting things up for the next iteration of the while loop \n",
    "        current_state = next_state\n",
    "        chosen_action = agent.select_epsilon_greedy_action(current_state)\n",
    "        \n",
    "        step += 1\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if(episode == num_episodes-1):\n",
    "            env.render()\n",
    "            \n",
    "        if(reward_sum < -300):\n",
    "            done = True\n",
    "            \n",
    "    steps_array[episode] = step    \n",
    "    \n",
    "    # We\n",
    "    agent.learn_from_m_random_transitions_in_replay_buffer(16)\n",
    "    rewardList.append(reward_sum)\n",
    "    episodeList.append(agent.epsilon)\n",
    "    scores_window.append(reward_sum)\n",
    "    scores_window1.append(reward_sum)\n",
    "    #agent.epsilon *= agent.epsilonDecay\n",
    "    agent.updateEpsilon()\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    #if(episode > 20):\n",
    "    #    meanRewards.append(np.mean(rewardList[-19:]))\n",
    "    meanRewards.append(np.mean(scores_window))\n",
    "    meanRewards1.append(np.mean(scores_window1))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    ax = plt.subplot(2,2,1)\n",
    "    #ax.plot(range(episode+1), rewardList)\n",
    "    ax2 = ax.twinx()\n",
    "    ax.plot(range(episode+1), rewardList, 'g-')\n",
    "    ax2.plot(range(episode+1), episodeList, 'b-')\n",
    "\n",
    "    ax = plt.subplot(2,2,2)\n",
    "    ax.plot(range(len(meanRewards)), meanRewards, label=\"100 mean\")\n",
    "    ax.plot(range(len(meanRewards1)), meanRewards1, label=\"20 mean\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Saving the trained model\n",
    "agent.q_network.save_weights(modelFileName)\n",
    "env.close()\n",
    "\n",
    "plt.plot(range(num_episodes), rewardList)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
