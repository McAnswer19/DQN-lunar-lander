{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:57:48.629072Z",
     "start_time": "2019-06-03T13:57:45.608356Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jack\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LeakyReLU\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:57:50.025126Z",
     "start_time": "2019-06-03T13:57:50.020108Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating Deep-Q network\n",
    "def q_network(numInput, numHidden, numHiddenLayers, numOutput, optimiserFunction=tf.train.AdamOptimizer, \\\n",
    "              alpha=0.00025, lossFunction=\"mse\", hiddenActivation=\"relu\", outputActivation=\"linear\"): \n",
    "    \n",
    "    #Creating a TensorFlow class\n",
    "    network = Sequential()\n",
    "    \n",
    "    #Adding 'n' hidden layers\n",
    "    for x in range(numHiddenLayers):\n",
    "        if x == 0:\n",
    "            #Creating first hidden layer\n",
    "            network.add(Dense(numHidden, input_dim=numInput, activation=hiddenActivation))\n",
    "        else:\n",
    "            network.add(Dense(numHidden, activation=hiddenActivation))\n",
    "        \n",
    "    #Creating output layer\n",
    "    network.add(Dense(numOutput, activation=outputActivation))\n",
    "    #network.add(Dense(numOutput, activation=LeakyReLU(alpha=0.1)))\n",
    "    \n",
    "    #Defining the loss function, the optimiser and the metrics.\n",
    "    network.compile(loss=lossFunction, optimizer=Adam(lr=alpha))\n",
    "    \n",
    "    return network   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:57:50.929828Z",
     "start_time": "2019-06-03T13:57:50.924840Z"
    }
   },
   "outputs": [],
   "source": [
    "class memory():\n",
    "    def __init__(self, maxSize=2000):\n",
    "        self.memory_buffer = deque(maxlen=maxSize)\n",
    "        self.memory_rewards = []\n",
    "        \n",
    "    def add(self, state, action, reward, state_prime, done):\n",
    "        self.memory_buffer.append((state, action, reward, state_prime, done))\n",
    "        #self.memory_rewards.append(reward)\n",
    "        \n",
    "    def getBuffer(self):\n",
    "        return self.memory_buffer\n",
    "    \n",
    "    def getRewards(self):\n",
    "        return self.memory_rewards\n",
    "    \n",
    "    def resetRewards(self):\n",
    "        self.memory_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T13:57:51.881679Z",
     "start_time": "2019-06-03T13:57:51.871702Z"
    }
   },
   "outputs": [],
   "source": [
    "class rocketMan():\n",
    "    def __init__(self, numInput, numHidden, numHiddenLayers, numOutput, modelFileName, \\\n",
    "                 alpha=0.0001, gamma=0.99, epsilon=0.8, epsilonEnd=0, epsilonDecay=0.995, batchSize=32, \\\n",
    "                 learningPoint=5000, load=False, save=False):\n",
    "    \n",
    "        self.numInput = numInput\n",
    "        self.numHidden = numHidden\n",
    "        self.numHiddenLayers = numHiddenLayers\n",
    "        self.numOutput = numOutput\n",
    "    \n",
    "        self.network = q_network(numInput, numHidden, numHiddenLayers, numOutput)\n",
    "        self.targetNetwork = q_network(numInput, numHidden, numHiddenLayers, numOutput)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilonEnd = epsilonEnd\n",
    "        self.epsilonDecay = epsilonDecay\n",
    "        self.batchSize = batchSize\n",
    "                            \n",
    "        self.replay = memory(250000) #Changing from 2000 to 500,000\n",
    "        \n",
    "        self.learningPoint = learningPoint\n",
    "        \n",
    "        #records the current number of steps taking through all episodes\n",
    "        self.steps = 0 \n",
    "        \n",
    "        #How often to soft update the target network\n",
    "        self.updateFreq = 500\n",
    "         \n",
    "        if(load):\n",
    "            self.targetNetwork.load_weights(modelFileName)\n",
    "        \n",
    "    def getAction(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return(env.action_space.sample())\n",
    "        else:\n",
    "            return(np.argmax(self.network.predict(state.reshape(1,-1))[0])) \n",
    "        \n",
    "    def addMemory(self, state, action, reward, state_prime, done):\n",
    "        self.replay.add(state, action, reward, state_prime, done)\n",
    "        \n",
    "        if(self.steps % self.updateFreq) == 0:\n",
    "            self.softWeightUpdate(0.5)\n",
    "        self.steps += 1\n",
    "        \n",
    "    def softWeightUpdate(self, tau):\n",
    "        weights = np.asarray(self.network.get_weights())\n",
    "        target_weights = np.asarray(self.targetNetwork.get_weights())\n",
    "        self.targetNetwork.set_weights((weights * tau) + (target_weights * (1 - tau)))\n",
    "        \n",
    "    def train(self):    \n",
    "        if(len(self.replay.getBuffer()) >= self.learningPoint):   \n",
    "            training, target = self.getTrainingDataset()\n",
    "            self.network.fit(training, target, batch_size=self.batchSize, nb_epoch=1, verbose=0)\n",
    "            self.softWeightUpdate(0.5)     \n",
    "        \n",
    "    def getTrainingDataset(self):\n",
    "        trainingX = []\n",
    "        trainingY = []\n",
    "        for state, action, reward, state_prime, done in random.sample(self.replay.getBuffer(), self.batchSize):\n",
    "            q_values = self.network.predict(state.reshape(1,-1))\n",
    "            target_q_values = q_values\n",
    "\n",
    "            if done == True: \n",
    "                target_q_values[0][action] = reward\n",
    "            else: \n",
    "                target_q_values[0][action] = reward + (self.gamma * np.max(self.targetNetwork.predict(state_prime.reshape(1,-1))))\n",
    "            \n",
    "            trainingX.append(state)\n",
    "            trainingY.append(target_q_values[0])\n",
    "        return np.asarray(trainingX), np.asarray(trainingY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T16:47:23.245219Z",
     "start_time": "2019-06-03T16:34:25.168041Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(hwnd, msg, wParam, lParam)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_window_proc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_handlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhwnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwParam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlParam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mevent_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent_handlers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6591afe864b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderEpsiode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mchosenAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrocketManAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetAction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polygon\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflagy2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mdispatch_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMSG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0m_user32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPeekMessageW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPM_REMOVE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m             \u001b[0m_user32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTranslateMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[0m_user32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDispatchMessageW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numEpisodes = 1000\n",
    "minReward = -300 #If the sum reward drops below this, stop\n",
    "\n",
    "#Repeatablilty\n",
    "seed = 32\n",
    "\n",
    "#Loading the gym environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Set seed for PRN generator of numpy, random module and gym env.\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "#Where to save the network weights\n",
    "modelFileName = \"LunarLanderWeights.h5\"\n",
    "\n",
    "numInput = env.observation_space.shape[0]\n",
    "numActions = env.action_space.n\n",
    "\n",
    "#Number of hidden neurons and layers0\n",
    "numHidden = 128\n",
    "numHiddenLayers = 1\n",
    "\n",
    "#Creating agent (rocketMan)\n",
    "rocketManAgent = rocketMan(numInput, numHidden, numHiddenLayers, numActions,modelFileName)\n",
    "\n",
    "#Stop if episode takes longer than this time\n",
    "maxTime = 10\n",
    "\n",
    "#Information for plotting\n",
    "rewardList = []\n",
    "meanRewards = []\n",
    "meanRewards1 = []\n",
    "epsilonList = []\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_window1 = deque(maxlen=20)\n",
    "\n",
    "#These are getting a bit confusing, rename.\n",
    "RTmean = []\n",
    "RTwindow = deque(maxlen=20)\n",
    "RTwindow1 = deque(maxlen=100)\n",
    "meanRTWindow = []\n",
    "meanRTWindow1 = []\n",
    "\n",
    "\n",
    "#Every 'n' steps, render the lander\n",
    "renderEvery = 100\n",
    "renderLander = True\n",
    "\n",
    "avgEpisodeRT = 0 #The average runtime for each episode\n",
    "saveRecordings = False #Do you want to save the file\n",
    "\n",
    "start = timer()\n",
    "for episodeNum in range(numEpisodes):\n",
    "    renderEpsiode = False #By default, do not render the episode\n",
    "    \n",
    "    \n",
    "    if ((episodeNum % renderEvery) == 0) and (renderLander):\n",
    "        renderEpsiode = True\n",
    "        # env = gym.wrappers.Monitor(env, 'recording/' + str(episodeNum) + '/', force=True) \n",
    "        \n",
    "    \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    episodeReward = 0\n",
    "    episodeStartTime = timer()\n",
    "    \n",
    "    \n",
    "    while True:       \n",
    "        if(renderEpsiode):\n",
    "            env.render()\n",
    "        \n",
    "        chosenAction = rocketManAgent.getAction(state)\n",
    "        \n",
    "        #Taking action in environemnt\n",
    "        state_prime, reward, done, info = env.step(chosenAction)\n",
    "        \n",
    "        #Adding the state information to the replay buffer\n",
    "        rocketManAgent.replay.add(state, chosenAction, reward, state_prime, done)\n",
    "        \n",
    "        state = state_prime\n",
    "        \n",
    "        #episode_rewards = np.sum(rocketManAgent.replay.getRewards())\n",
    "        episodeReward += reward\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        #if(step % 16) == 0:\n",
    "        rocketManAgent.train()\n",
    "        \n",
    "        if(episodeReward < minReward):\n",
    "            done = True\n",
    "            \n",
    "        #Some iterations are taking over 30 seconds, far too long \n",
    "        #episodeTempEndTime = timer()\n",
    "        \n",
    "        #if(episodeTempEndTime - episodeStartTime) > maxTime:\n",
    "        #    done = True\n",
    "            \n",
    "        if done == True:\n",
    "            #rocketManAgent.train()\n",
    "            \n",
    "            #print(\"Test:\", len(rocketManAgent.replay.getBuffer()))\n",
    "            \n",
    "            \n",
    "            #avgEpisodeRT += (episodeEndTime - episodeStartTime)\n",
    "            \n",
    "            #Updating epsilon\n",
    "            if rocketManAgent.epsilon >= rocketManAgent.epsilonEnd:\n",
    "                rocketManAgent.epsilon *= rocketManAgent.epsilonDecay   \n",
    "            \n",
    "            rewardList.append(episodeReward)\n",
    "            epsilonList.append(rocketManAgent.epsilon)\n",
    "            scores_window.append(episodeReward)\n",
    "            scores_window1.append(episodeReward)\n",
    "            \n",
    "            meanRewards.append(np.mean(scores_window))\n",
    "            meanRewards1.append(np.mean(scores_window1))\n",
    "            \n",
    "            episodeEndTime = timer()\n",
    "            \n",
    "            if not renderEpsiode:\n",
    "                epRT = (episodeEndTime - episodeStartTime)\n",
    "                RTmean.append(epRT)\n",
    "                RTwindow.append(epRT)\n",
    "                RTwindow1.append(epRT)\n",
    "                meanRTWindow.append(np.mean(RTwindow))\n",
    "                meanRTWindow1.append(np.mean(RTwindow1))\n",
    "                    \n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.subplots_adjust(top=1.0)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            ax = plt.subplot(2,2,1)\n",
    "            ax2 = ax.twinx()\n",
    "            ax.set_title(\"Epsilon & Reward vs Episode\")\n",
    "            ax.set_ylabel(\"Reward\")\n",
    "            ax2.set_ylabel(\"Epsilon\")\n",
    "            ax2.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            ax.set_xlabel(\"Episode (Ep)\")\n",
    "            lns1 = ax.plot(range(episodeNum+1), rewardList, 'g-', label=\"Reward\")\n",
    "            lns2 = ax2.plot(range(episodeNum+1), epsilonList, 'b-', label=\"Epsilon\")\n",
    "            \n",
    "            lns = lns1+lns2\n",
    "            labs = [l.get_label() for l in lns]\n",
    "            ax2.legend(lns, labs, loc=2)\n",
    "\n",
    "            ax = plt.subplot(2,2,2)\n",
    "            ax.set_title(\"Simple Moving Average (SMA) vs Episode\")\n",
    "            ax.set_ylabel(\"SMA reward\")\n",
    "            ax.set_xlabel(\"Episode (Ep)\")            \n",
    "            ax.plot(range(len(meanRewards1)), meanRewards1, color='orange', label=\"20 Ep SMA\")\n",
    "            ax.plot(range(len(meanRewards)), meanRewards, color='blue', label=\"100 Ep SMA\")\n",
    "            plt.legend(loc=2)\n",
    "            \n",
    "            if(len(RTmean) > 0):\n",
    "                ax = plt.subplot(2,2,3)   \n",
    "                ax.set_title(\"Runtime (RT) & SMA vs Episode\")\n",
    "                ax.set_ylabel(\"Runtime/s (RT)\")\n",
    "                ax.set_xlabel(\"Episode (Ep)\")\n",
    "                \n",
    "                ax.plot((range(len(RTmean))), RTmean, color='r', label=\"Current RT\", linewidth=0.5)\n",
    "                plt.axhline(y=RTmean[-1], color='r', linestyle='--')\n",
    "                \n",
    "                ax.plot((range(len(meanRTWindow))), meanRTWindow, color='orange', label=\"20 Ep SMA\", linewidth=3.0)\n",
    "                plt.axhline(y=meanRTWindow[-1], color='orange', linestyle='--')\n",
    "\n",
    "                ax.plot((range(len(meanRTWindow1))), meanRTWindow1, color='blue', label=\"100 EP SMA\", linewidth=3.0)\n",
    "                plt.axhline(y=meanRTWindow1[-1], color='blue', linestyle='--')\n",
    "                            \n",
    "                plt.legend(loc=2)\n",
    "            plt.savefig('plotImages/' + str(episodeNum) + '.png')\n",
    "            plt.show()                      \n",
    "            \n",
    "            #Resetting epsiode rewards\n",
    "            #rocketManAgent.replay.resetRewards()\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            \n",
    "            #print(\"Epsiode RT: \", (episodeEndTime - episodeStartTime), \" | Average: \", (avgEpisodeRT/(episodeNum+1)))\n",
    "            break\n",
    "    env.close()\n",
    "end = timer()\n",
    "\n",
    "#Creating video\n",
    "os.system(\"ffmpeg -f image2 -r 12 -i plotImages/%d.png -y -an recording/timelapse.mp4\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Delete files in folder\n",
    "import glob\n",
    "\n",
    "files = glob.glob('plotImages/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "    \n",
    "print(\"RT: \", (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes for next time\n",
    "#https://stackoverflow.com/questions/8827016/matplotlib-savefig-in-jpeg-format\n",
    "#https://stackoverflow.com/questions/753190/programmatically-generate-video-or-animated-gif-in-python\n",
    "#https://github.com/openai/gym/issues/1254    \n",
    "#https://moodle.bath.ac.uk/pluginfile.php/1100399/course/section/175216/rl18.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
