{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LeakyReLU\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Deep-Q network\n",
    "def q_network(numInput, numHidden, numHiddenLayers, numOutput, optimiserFunction=tf.train.AdamOptimizer, \\\n",
    "              alpha=0.00025, lossFunction=\"mse\", hiddenActivation=\"relu\", outputActivation=\"linear\"): \n",
    "    \n",
    "    #Creating a TensorFlow class\n",
    "    network = Sequential()\n",
    "    network.add(Dense(64, activation=hiddenActivation))\n",
    "    \n",
    "    #Adding 'n' hidden layers\n",
    "    for _ in range(numHiddenLayers):\n",
    "        network.add(Dense(numHidden, activation=hiddenActivation))\n",
    "        \n",
    "    #Creating output layer\n",
    "    network.add(Dense(numOutput, activation=outputActivation))\n",
    "    #network.add(Dense(numOutput, activation=LeakyReLU(alpha=0.1)))\n",
    "    \n",
    "    #Defining the loss function, the optimiser and the metrics.\n",
    "    network.compile(loss=lossFunction, optimizer=Adam(lr=alpha))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class memory():\n",
    "    def __init__(self, maxSize=500000):\n",
    "        self.memory_buffer = deque(maxlen=maxSize)\n",
    "        self.memory_rewards = []\n",
    "        \n",
    "    def add(self, state, action, reward, state_prime, done):\n",
    "        self.memory_buffer.append((state, action, reward, state_prime, done))\n",
    "        self.memory_rewards.append(reward)\n",
    "        \n",
    "    def getBuffer(self):\n",
    "        return self.memory_buffer\n",
    "    \n",
    "    def getRewards(self):\n",
    "        return self.memory_rewards\n",
    "    \n",
    "    def resetRewards(self):\n",
    "        self.memory_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rocketMan():\n",
    "    def __init__(self, numInput, numHidden, numHiddenLayers, numOutput, \\\n",
    "                 alpha=0.0001, gamma=0.99, epsilon= 1, epsilonEnd=0.01, epsilonDecay=0.99, batchSize=64):\n",
    "    \n",
    "        self.network = q_network(numInput, numHidden, numHiddenLayers, numOutput)\n",
    "        self.targetNetwork = q_network(numInput, numHidden, numHiddenLayers, numOutput)\n",
    "        \n",
    "        self.numInput = numInput\n",
    "        self.numHidden = numHidden\n",
    "        self.numHiddenLayers = numHiddenLayers\n",
    "        self.numOutput = numOutput\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilonEnd = epsilonEnd\n",
    "        self.epsilonDecay = epsilonDecay\n",
    "        self.batchSize = batchSize\n",
    "                            \n",
    "        self.replay = memory(500000) #Changed from 2000 to 500,000\n",
    "        \n",
    "        self.minsamples=65\n",
    "        self.steps = 0\n",
    "        self.update_target_freq = 600\n",
    "        \n",
    "    def getAction(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return(env.action_space.sample())\n",
    "        else:\n",
    "            return(np.argmax(self.network.predict(state.reshape(1,-1))[0])) \n",
    "        \n",
    "    def addMemory(self, state, action, reward, state_prime, done):\n",
    "        self.replay.add(state, action, reward, state_prime, done)\n",
    "        \n",
    "        if self.steps % self.update_target_freq == 0:\n",
    "            self.softWeightUpdate(0.5)\n",
    "        self.steps += 1\n",
    "        \n",
    "    def softWeightUpdate(self, tau):\n",
    "        weights = np.asarray(self.network.get_weights())\n",
    "        target_weights = np.asarray(self.targetNetwork.get_weights())\n",
    "        self.targetNetwork.set_weights( (weights * tau) + (target_weights * (1 - tau)))\n",
    "        \n",
    "    def train(self):\n",
    "        if(len(self.replay.getBuffer()) >= self.minsamples):               \n",
    "            targetX, targetY = self.getTrainingSet()\n",
    "            \n",
    "            self.network.fit(targetX, targetY, batch_size=self.batchSize, nb_epoch= 1, verbose=0)\n",
    "            self.softWeightUpdate(0.5)\n",
    "            \n",
    "    def getTrainingSet(self):\n",
    "        stateArray, actionArray, rewardArray, state_primeArray, doneArray = \\\n",
    "                    zip(*random.sample(self.replay.getBuffer(), self.batchSize))\n",
    "        \n",
    "        stateArray = np.array(stateArray)\n",
    "        state_primeArray = np.array(state_primeArray)\n",
    "        \n",
    "        currentQ = self.network.predict(stateArray)\n",
    "        targetQ = self.targetNetwork.predict(state_primeArray)\n",
    "        \n",
    "        outputX = stateArray\n",
    "        outputY = [] #Currently a list but will convert to np array\n",
    "        \n",
    "        for reward, action, done, q, targetQ in zip(rewardArray, actionArray, doneArray, currentQ, targetQ):\n",
    "            target_for_action = reward # correct if state is final.            \n",
    "            \n",
    "            if not done:\n",
    "                #If not add to it the discounted future rewards per current policy                \n",
    "                target_for_action += (self.gamma * max(targetQ))\n",
    "                       \n",
    "            q[action] = target_for_action\n",
    "            outputY.append(q)\n",
    "            \n",
    "        return (outputX, np.array(outputY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpisodes = 1000\n",
    "minReward = -300 #If the sum reward drops below this, stop\n",
    "\n",
    "#Repeatablilty\n",
    "seed = 32\n",
    "\n",
    "#Loading the gym environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Set seed for PRN generator of numpy, random module and gym env.\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "numInput = env.observation_space.shape[0]\n",
    "numActions = env.action_space.n\n",
    "\n",
    "#Number of hidden neurons and layers0\n",
    "numHidden = 128\n",
    "numHiddenLayers = 1 #Already has a hidden layer, so 1 is actually 2 hidden (must change this)\n",
    "\n",
    "#Creating agent (rocketMan)\n",
    "rocketManAgent = rocketMan(numInput, numHidden, numHiddenLayers, numActions)\n",
    "\n",
    "#Information for plotting\n",
    "rewardList = []\n",
    "meanRewards = []\n",
    "meanRewards1 = []\n",
    "epsilonList = []\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_window1 = deque(maxlen=20)\n",
    "\n",
    "for episodeNum in range(numEpisodes):\n",
    "    \n",
    "    \n",
    "    # Past episode 300, don't do random actions anymore. There has been enough explanation. \n",
    "    if episodeNum > 300: \n",
    "        rocketManAgent.epsilon = 0\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    while True:        \n",
    "        chosenAction = rocketManAgent.getAction(state)\n",
    "        \n",
    "        #Taking action in environemnt\n",
    "        state_prime, reward, done, info = env.step(chosenAction)\n",
    "        \n",
    "        #Adding the state information to the replay buffer\n",
    "        rocketManAgent.replay.add(state, chosenAction, reward, state_prime, done)\n",
    "        \n",
    "        state = state_prime\n",
    "        \n",
    "        episode_rewards = np.sum(rocketManAgent.replay.getRewards())\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        #if(step % 16) == 0:\n",
    "        rocketManAgent.train()\n",
    "        \n",
    "        if(episode_rewards < minReward):\n",
    "            done = True\n",
    "            \n",
    "        if done == True:\n",
    "            #rocketManAgent.train()\n",
    "            \n",
    "            #print(\"Test:\", len(rocketManAgent.replay.getBuffer()))\n",
    "            \n",
    "            #Updating epsilon\n",
    "            if rocketManAgent.epsilon >= rocketManAgent.epsilonEnd:\n",
    "                rocketManAgent.epsilon *= rocketManAgent.epsilonDecay   \n",
    "            \n",
    "            rewardList.append(episode_rewards)\n",
    "            epsilonList.append(rocketManAgent.epsilon)\n",
    "            scores_window.append(episode_rewards)\n",
    "            scores_window1.append(episode_rewards)\n",
    "            \n",
    "            meanRewards.append(np.mean(scores_window))\n",
    "            meanRewards1.append(np.mean(scores_window1))\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            plt.figure(figsize=(20, 10))\n",
    "            ax = plt.subplot(2,2,1)\n",
    "            ax2 = ax.twinx()\n",
    "            ax.plot(range(episodeNum+1), rewardList, 'g-')\n",
    "            ax2.plot(range(episodeNum+1), epsilonList, 'b-')\n",
    "\n",
    "            ax = plt.subplot(2,2,2)\n",
    "            ax.plot(range(len(meanRewards)), meanRewards, label=\"100 mean\")\n",
    "            ax.plot(range(len(meanRewards1)), meanRewards1, label=\"20 mean\")\n",
    "            plt.legend()\n",
    "            plt.show()                      \n",
    "            \n",
    "            #Resetting epsiode rewards\n",
    "            rocketManAgent.replay.resetRewards()\n",
    "            \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### numEpisodes = 1000\n",
    "minReward = -300 #If the sum reward drops below this, stop\n",
    "\n",
    "#Repeatablilty\n",
    "seed = 32\n",
    "\n",
    "#Loading the gym environment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Set seed for PRN generator of numpy, random module and gym env.\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "#Where to save the network weights\n",
    "modelFileName = \"LunarLanderWeights.h5\"\n",
    "\n",
    "numInput = env.observation_space.shape[0]\n",
    "numActions = env.action_space.n\n",
    "\n",
    "#Number of hidden neurons and layers0\n",
    "numHidden = 64\n",
    "numHiddenLayers = 2\n",
    "\n",
    "#Creating agent (rocketMan)\n",
    "rocketManAgent = rocketMan(numInput, numHidden, numHiddenLayers, numActions,modelFileName)\n",
    "\n",
    "#Stop if episode takes longer than this time\n",
    "maxTime = 10\n",
    "\n",
    "#Information for plotting\n",
    "rewardList = []\n",
    "meanRewards = []\n",
    "meanRewards1 = []\n",
    "epsilonList = []\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_window1 = deque(maxlen=20)\n",
    "\n",
    "#These are getting a bit confusing, rename.\n",
    "RTmean = []\n",
    "RTwindow = deque(maxlen=20)\n",
    "RTwindow1 = deque(maxlen=100)\n",
    "meanRTWindow = []\n",
    "meanRTWindow1 = []\n",
    "\n",
    "\n",
    "#Every 'n' steps, render the lander\n",
    "renderEvery = 100\n",
    "renderLander = False\n",
    "\n",
    "avgEpisodeRT = 0 #The average runtime for each episode\n",
    "saveRecordings = False #Do you want to save the file\n",
    "\n",
    "start = timer()\n",
    "for episodeNum in range(numEpisodes):\n",
    "    renderEpsiode = False #By default, do not render the episode\n",
    "    \n",
    "    \n",
    "    if ((episodeNum % renderEvery) == 0) and (renderLander):\n",
    "        renderEpsiode = True\n",
    "        env = gym.wrappers.Monitor(env, 'recording/' + str(episodeNum) + '/', force=True) \n",
    "        \n",
    "    \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    episodeReward = 0\n",
    "    episodeStartTime = timer()\n",
    "    \n",
    "    \n",
    "    while True:       \n",
    "        if(renderEpsiode):\n",
    "            env.render()\n",
    "        \n",
    "        chosenAction = rocketManAgent.getAction(state)\n",
    "        \n",
    "        #Taking action in environemnt\n",
    "        state_prime, reward, done, info = env.step(chosenAction)\n",
    "        \n",
    "        #Adding the state information to the replay buffer\n",
    "        rocketManAgent.replay.add(state, chosenAction, reward, state_prime, done)\n",
    "        \n",
    "        state = state_prime\n",
    "        \n",
    "        #episode_rewards = np.sum(rocketManAgent.replay.getRewards())\n",
    "        episodeReward += reward\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        #if(step % 16) == 0:\n",
    "        rocketManAgent.train()\n",
    "        \n",
    "        if(episodeReward < minReward):\n",
    "            done = True\n",
    "            \n",
    "        #Some iterations are taking over 30 seconds, far too long \n",
    "        #episodeTempEndTime = timer()\n",
    "        \n",
    "        #if(episodeTempEndTime - episodeStartTime) > maxTime:\n",
    "        #    done = True\n",
    "            \n",
    "        if done == True:\n",
    "            #rocketManAgent.train()\n",
    "            \n",
    "            #print(\"Test:\", len(rocketManAgent.replay.getBuffer()))\n",
    "            \n",
    "            \n",
    "            #avgEpisodeRT += (episodeEndTime - episodeStartTime)\n",
    "            \n",
    "            #Updating epsilon\n",
    "            if rocketManAgent.epsilon >= rocketManAgent.epsilonEnd:\n",
    "                rocketManAgent.epsilon *= rocketManAgent.epsilonDecay   \n",
    "            \n",
    "            rewardList.append(episodeReward)\n",
    "            epsilonList.append(rocketManAgent.epsilon)\n",
    "            scores_window.append(episodeReward)\n",
    "            scores_window1.append(episodeReward)\n",
    "            \n",
    "            meanRewards.append(np.mean(scores_window))\n",
    "            meanRewards1.append(np.mean(scores_window1))\n",
    "            \n",
    "            episodeEndTime = timer()\n",
    "            \n",
    "            if not renderEpsiode:\n",
    "                epRT = (episodeEndTime - episodeStartTime)\n",
    "                RTmean.append(epRT)\n",
    "                RTwindow.append(epRT)\n",
    "                RTwindow1.append(epRT)\n",
    "                meanRTWindow.append(np.mean(RTwindow))\n",
    "                meanRTWindow1.append(np.mean(RTwindow1))\n",
    "                    \n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            plt.figure(figsize=(20, 12))            \n",
    "            plt.subplots_adjust(top=0.88)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            ax = plt.subplot(2,2,1)\n",
    "            ax2 = ax.twinx()\n",
    "            ax.set_title(\"Epsilon & Reward vs Episode\")\n",
    "            ax.set_ylabel(\"Reward\")\n",
    "            ax2.set_ylabel(\"Epsilon\")\n",
    "            ax2.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "            ax.set_xlabel(\"Episode (Ep)\")\n",
    "            lns1 = ax.plot(range(episodeNum+1), rewardList, 'g-', label=\"Reward\")\n",
    "            lns2 = ax2.plot(range(episodeNum+1), epsilonList, 'b-', label=\"Epsilon\")\n",
    "            \n",
    "            lns = lns1+lns2\n",
    "            labs = [l.get_label() for l in lns]\n",
    "            ax2.legend(lns, labs, loc=2)\n",
    "\n",
    "            ax = plt.subplot(2,2,2)\n",
    "            ax.set_title(\"Simple Moving Average (SMA) vs Episode\")\n",
    "            ax.set_ylabel(\"SMA reward\")\n",
    "            ax.set_xlabel(\"Episode (Ep)\")            \n",
    "            ax.plot(range(len(meanRewards1)), meanRewards1, color='orange', label=\"20 Ep SMA\")\n",
    "            ax.plot(range(len(meanRewards)), meanRewards, color='blue', label=\"100 Ep SMA\")\n",
    "            plt.legend(loc=2)\n",
    "            \n",
    "            if(len(RTmean) > 0):\n",
    "                ax = plt.subplot(2,2,3)   \n",
    "                ax.set_title(\"Runtime (RT) & SMA vs Episode\")\n",
    "                ax.set_ylabel(\"Runtime/s (RT)\")\n",
    "                ax.set_xlabel(\"Episode (Ep)\")\n",
    "                \n",
    "                ax.plot((range(len(RTmean))), RTmean, color='r', label=\"Current RT\", linewidth=0.5)\n",
    "                plt.axhline(y=RTmean[-1], color='r', linestyle='--')\n",
    "                \n",
    "                ax.plot((range(len(meanRTWindow))), meanRTWindow, color='orange', label=\"20 Ep SMA\", linewidth=3.0)\n",
    "                plt.axhline(y=meanRTWindow[-1], color='orange', linestyle='--')\n",
    "\n",
    "                ax.plot((range(len(meanRTWindow1))), meanRTWindow1, color='blue', label=\"100 Ep SMA\", linewidth=3.0)\n",
    "                plt.axhline(y=meanRTWindow1[-1], color='blue', linestyle='--')\n",
    "                            \n",
    "                plt.legend(loc=2)\n",
    "            plt.savefig('plotImages/' + str(episodeNum) + '.png')\n",
    "            plt.show()                      \n",
    "            \n",
    "            #Resetting epsiode rewards\n",
    "            #rocketManAgent.replay.resetRewards()\n",
    "            \n",
    "            env.close()\n",
    "            \n",
    "            \n",
    "            #print(\"Epsiode RT: \", (episodeEndTime - episodeStartTime), \" | Average: \", (avgEpisodeRT/(episodeNum+1)))\n",
    "            break\n",
    "    env.close()\n",
    "end = timer()\n",
    "\n",
    "#Creating video\n",
    "os.system(\"ffmpeg -f image2 -r 24 -i plotImages/%d.png -y -an recording/timelapse.mp4\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Delete files in folder\n",
    "import glob\n",
    "\n",
    "files = glob.glob('plotImages/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "    \n",
    "print(\"RT: \", (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
